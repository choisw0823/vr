"""
MSRVTT ì „ì²´ ë¹„ë””ì˜¤ ë° í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸
- ëª¨ë“  ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (10,000ê°œ)
- ëª¨ë“  í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ (í•™ìŠµìš© + í…ŒìŠ¤íŠ¸ìš©)
- ë‚˜ì¤‘ì— ë¹ ë¥¸ similarity ê³„ì‚°ì„ ìœ„í•´ ì €ì¥
"""

import json
import logging
import os
import sys
import pickle
import ast
from collections import defaultdict
from tqdm import tqdm

import numpy as np
import torch
import torch.backends.cudnn as cudnn
import torch.distributed as dist
from torch.utils.data import DataLoader, Dataset

from dataset import pt_dataset, ret_dataset
from models.internvideo2_stage2_audiovisual import InternVideo2_Stage2_audiovisual
from models.criterions import get_sim
from tasks.shared_utils import setup_model
from utils.config_utils import setup_main
from utils.distributed import get_rank, is_main_process

logger = logging.getLogger(__name__)

class MSRVTTFeatureDataset(Dataset):
    """MSRVTT ì „ì²´ ë°ì´í„°ì…‹ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹"""
    
    def __init__(self, video_root, annotation_files, tokenizer, max_txt_length=77):
        self.video_root = video_root
        self.tokenizer = tokenizer
        self.max_txt_length = max_txt_length
        
        # ëª¨ë“  annotation íŒŒì¼ ë¡œë“œ
        self.video_data = {}  # video_id -> video_info
        self.text_data = []   # ëª¨ë“  í…ìŠ¤íŠ¸ ì¿¼ë¦¬
        
        for ann_file in annotation_files:
            logger.info(f"Loading annotation file: {ann_file}")
            with open(ann_file, 'r') as f:
                data = json.load(f)
            
            for item in data:
                video_id = item['video_id']
                video_path = os.path.join(video_root, item['video'])
                
                # ë¹„ë””ì˜¤ ì •ë³´ ì €ì¥ (ì¤‘ë³µ ì œê±°)
                if video_id not in self.video_data:
                    self.video_data[video_id] = {
                        'video_id': video_id,
                        'video_path': video_path,
                        'video_file': item['video']
                    }
                
                # í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ì €ì¥
                text_item = {
                    'video_id': video_id,
                    'caption': item['caption'],
                    'source': item.get('source', 'MSR-VTT'),
                    'category': item.get('category', 0),
                    'split': self._get_split_from_filename(ann_file)
                }
                self.text_data.append(text_item)
        
        self.video_ids = list(self.video_data.keys())
        logger.info(f"Total videos: {len(self.video_ids)}")
        logger.info(f"Total text queries: {len(self.text_data)}")
    
    def _get_split_from_filename(self, filename):
        """íŒŒì¼ëª…ì—ì„œ split ì •ë³´ ì¶”ì¶œ"""
        if 'test' in filename:
            return 'test'
        elif 'train_7k' in filename:
            return 'train_7k'
        elif 'train_9k' in filename:
            return 'train_9k'
        else:
            return 'unknown'
    
    def __len__(self):
        return len(self.video_ids)
    
    def get_video_info(self, idx):
        """ë¹„ë””ì˜¤ ì •ë³´ ë°˜í™˜"""
        video_id = self.video_ids[idx]
        return self.video_data[video_id]
    
    def get_texts_for_video(self, video_id):
        """íŠ¹ì • ë¹„ë””ì˜¤ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ì¿¼ë¦¬ ë°˜í™˜"""
        return [item for item in self.text_data if item['video_id'] == video_id]

def extract_video_features(model, video_path, device, config):
    """ë‹¨ì¼ ë¹„ë””ì˜¤ì˜ íŠ¹ì§• ì¶”ì¶œ - ê¸°ì¡´ retrieval_utils ì‚¬ìš©"""
    try:
        # ê¸°ì¡´ evalê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤ ë¡œë“œ
        from dataset.video_utils import VIDEO_READER_FUNCS
        
        # ë¹„ë””ì˜¤ íŒŒì¼ ì½ê¸° (ì˜¬ë°”ë¥¸ ì¸ì ì „ë‹¬)
        video_reader = VIDEO_READER_FUNCS['decord']
        video_input = config.inputs.video_input
        
        # num_framesë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
        num_frames = int(video_input.num_frames) if hasattr(video_input, 'num_frames') else 8
        logger.info(f"Loading video {video_path} with {num_frames} frames")
        
        video_data = video_reader(video_path, num_frames, sample='rand', fix_start=None, max_num_frames=-1, client=None, trimmed30=False)
        
        # video_dataëŠ” tupleì´ê³  ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë¹„ë””ì˜¤ í…ì„œ (T, 3, H, W)
        if isinstance(video_data, tuple):
            frames = video_data[0]  # (T, 3, H, W)
        elif isinstance(video_data, torch.Tensor):
            frames = video_data
        else:
            frames = torch.tensor(video_data)
        
        # ê¸°ì¡´ evalê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬: PIL Imageë¡œ ë³€í™˜ í›„ transform ì ìš©
        from PIL import Image
        from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode
        
        transform = Compose([
            Resize(224, interpolation=InterpolationMode.BICUBIC),
            CenterCrop(224),
            lambda image: image.convert("RGB"),
            ToTensor(),
            Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
        ])
        
        # ê° í”„ë ˆì„ì„ PIL Imageë¡œ ë³€í™˜ í›„ transform ì ìš©
        processed_frames = []
        for i in range(frames.shape[0]):
            frame = frames[i].permute(1, 2, 0)  # (3, H, W) -> (H, W, 3)
            frame_np = (frame * 255).byte().numpy()  # uint8ë¡œ ë³€í™˜
            pil_image = Image.fromarray(frame_np)
            transformed_frame = transform(pil_image)
            processed_frames.append(transformed_frame)
        
        # í…ì„œë¡œ ìŠ¤íƒí•˜ê³  ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        video_tensor = torch.stack(processed_frames).unsqueeze(0).to(device)  # (1, T, 3, 224, 224)
        
        with torch.no_grad():
            # ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ - ê¸°ì¡´ evalê³¼ ë™ì¼ (extract_vision_feats ë°©ì‹)
            # DistributedDataParallelë¡œ ë˜í•‘ëœ ê²½ìš° model.module ì‚¬ìš©
            actual_model = model.module if hasattr(model, 'module') else model
            vision_feat, pooled_vision_feat = actual_model.encode_vision(video_tensor, test=True)
            video_proj = actual_model.vision_proj(pooled_vision_feat)
            
        return video_proj.cpu()
        
    except Exception as e:
        logger.warning(f"Failed to extract features for {video_path}: {e}")
        return None

def extract_text_features(model, texts, tokenizer, device, video_feature=None):
    """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ íŠ¹ì§• ì¶”ì¶œ - ê·¹ë„ ë©”ëª¨ë¦¬ ì ˆì•½ ë²„ì „"""
    from tasks.retrieval_utils import extract_text_feats
    
    try:
        # ê¸°ì¡´ evalê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ
        # DistributedDataParallelë¡œ ë˜í•‘ëœ ê²½ìš° model.module ì‚¬ìš©
        actual_model = model.module if hasattr(model, 'module') else model
        
        # ë””ë²„ê¹…: í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ (ê°„ì†Œí™”)
        logger.info(f"Extracting text features for {len(texts)} texts")
        
        # ê·¹ë„ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í•˜ë‚˜ì”© ì²˜ë¦¬
        all_text_proj = []
        
        for i, text in enumerate(texts):
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # í•˜ë‚˜ì”© ì²˜ë¦¬
            result = extract_text_feats([text], 77, tokenizer, actual_model, device)
            
            # ë°˜í™˜ê°’ì´ tupleì¸ì§€ í™•ì¸í•˜ê³  ì ì ˆíˆ ì²˜ë¦¬
            if isinstance(result, tuple):
                if len(result) == 2:
                    text_feats, text_atts = result
                elif len(result) == 1:
                    text_feats = result[0]
                    text_atts = None
                else:
                    logger.warning(f"Unexpected tuple length: {len(result)}")
                    continue
            else:
                text_feats = result
                text_atts = None
            
            # projection í›„ ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ evalê³¼ ë™ì¼)
            text_proj = actual_model.text_proj(text_feats[:, 0])  # CLS tokenë§Œ ì‚¬ìš©
            
            # CPUë¡œ ì´ë™í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
            all_text_proj.append(text_proj.cpu())
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del text_feats, text_proj
            if text_atts is not None:
                del text_atts
            torch.cuda.empty_cache()
        
        if all_text_proj:
            # ëª¨ë“  ë°°ì¹˜ ê²°ê³¼ë¥¼ í•©ì¹˜ê¸°
            final_text_proj = torch.cat(all_text_proj, dim=0)
            logger.info(f"Final text_proj shape: {final_text_proj.shape}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del all_text_proj
            torch.cuda.empty_cache()
            
            return final_text_proj
        else:
            return None
            
    except Exception as e:
        logger.warning(f"Failed to extract text features: {e}")
        return None

def _load_single_video_feature(video_feat_file, video_id):
    """ë‹¨ì¼ ë¹„ë””ì˜¤ íŠ¹ì§•ë§Œ ë¡œë“œ"""
    try:
        with open(video_feat_file, 'rb') as f:
            all_video_features = pickle.load(f)
        
        # ë‹¨ì¼ ë¹„ë””ì˜¤ íŠ¹ì§•ë§Œ ë°˜í™˜
        if video_id in all_video_features:
            feature = all_video_features[video_id]
            # ì›ë³¸ ë°ì´í„° í•´ì œ
            del all_video_features
            return feature
        else:
            del all_video_features
            return None
    except Exception as e:
        logger.warning(f"Failed to load video feature for {video_id}: {e}")
        return None

def _load_video_features_lazy(video_feat_file, video_ids):
    """ë¹„ë””ì˜¤ íŠ¹ì§•ì„ lazy loadingìœ¼ë¡œ í•„ìš”í•œ ê²ƒë§Œ ë¡œë“œ"""
    try:
        with open(video_feat_file, 'rb') as f:
            all_video_features = pickle.load(f)
        
        # í•„ìš”í•œ ë¹„ë””ì˜¤ íŠ¹ì§•ë§Œ ë°˜í™˜
        needed_features = {}
        for video_id in video_ids:
            if video_id in all_video_features:
                needed_features[video_id] = all_video_features[video_id]
        
        # ì›ë³¸ ë°ì´í„° í•´ì œ
        del all_video_features
        return needed_features
    except Exception as e:
        logger.warning(f"Failed to load video features: {e}")
        return {}

def _save_single_text_feature(video_id, features, texts, output_dir):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¦‰ì‹œ ì €ì¥"""
    feature_file = os.path.join(output_dir, f"text_features_{video_id}.pkl")
    metadata_file = os.path.join(output_dir, f"text_metadata_{video_id}.pkl")
    
    with open(feature_file, 'wb') as f:
        pickle.dump({video_id: features}, f)
    with open(metadata_file, 'wb') as f:
        pickle.dump({video_id: texts}, f)

def _save_intermediate_results(features, metadata, output_dir, prefix):
    """ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)"""
    if not features:
        return
    
    # ì¤‘ê°„ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    intermediate_dir = os.path.join(output_dir, "intermediate")
    os.makedirs(intermediate_dir, exist_ok=True)
    
    # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
    import time
    timestamp = int(time.time())
    
    feat_file = os.path.join(intermediate_dir, f"{prefix}_features_{timestamp}.pkl")
    meta_file = os.path.join(intermediate_dir, f"{prefix}_metadata_{timestamp}.pkl")
    
    with open(feat_file, 'wb') as f:
        pickle.dump(features, f)
    with open(meta_file, 'wb') as f:
        pickle.dump(metadata, f)
    
    logger.info(f"Intermediate {prefix} features saved: {len(features)} items")

def main(config):
    # Setup basic logging
    logging.basicConfig(level=logging.INFO)
    logger.info(f"Config: {config}")
    
    # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí• ì§€ í™•ì¸ (ëª…ë ¹í–‰ ì¸ìë¡œ ì œì–´)
    text_only_mode = getattr(config, 'text_only', False)
    logger.info(f"Text-only mode: {text_only_mode}")
    
    # Setup model
    model_cls = InternVideo2_Stage2_audiovisual
    model, model_without_ddp, optimizer, scheduler, scaler, tokenizer, start_epoch, global_step = setup_model(
        config, model_cls, pretrain=False, find_unused_parameters=config.model.get("find_unused_parameters", False)
    )
    
    model.eval()
    
    # MSRVTT annotation íŒŒì¼ë“¤
    annotation_files = [
        "/home/work/smoretalk/seo/reranking/dataset/MSR-VTT/msrvtt_test_1k.json",
        "/home/work/smoretalk/seo/reranking/dataset/MSR-VTT/msrvtt_train_9k.json"
    ]
    
    video_root = "/home/work/smoretalk/seo/reranking/dataset/MSR-VTT/video"
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = MSRVTTFeatureDataset(video_root, annotation_files, tokenizer)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    output_dir = config.output_dir if hasattr(config, 'output_dir') else './outputs/msrvtt_features'
    os.makedirs(output_dir, exist_ok=True)
    
    logger.info("Starting feature extraction...")
    
    # ë¹„ë””ì˜¤ íŠ¹ì§• ì¶”ì¶œ (ë°°ì¹˜ë³„ ì²˜ë¦¬) - text_only_modeê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ
    if not text_only_mode:
        logger.info("Extracting video features for all videos (batch processing)...")
        video_features = {}
        video_metadata = {}
        
        batch_size = 1000  # ë°°ì¹˜ í¬ê¸°
        total_videos = len(dataset)
        
        for batch_start in tqdm(range(0, total_videos, batch_size), desc="Processing video batches"):
            batch_end = min(batch_start + batch_size, total_videos)
            logger.info(f"Processing videos {batch_start}-{batch_end-1}...")
            
            for i in range(batch_start, batch_end):
                video_info = dataset.get_video_info(i)
                video_id = video_info['video_id']
                video_path = video_info['video_path']
                
                if os.path.exists(video_path):
                    features = extract_video_features(model, video_path, config.device, config)
                    if features is not None:
                        video_features[video_id] = features
                        video_metadata[video_id] = video_info
                else:
                    logger.warning(f"Video file not found: {video_path}")
            
            # ë°°ì¹˜ë³„ ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if batch_start % (batch_size * 5) == 0:  # 5ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
                logger.info(f"Intermediate save at batch {batch_start//batch_size}")
                _save_intermediate_results(video_features, video_metadata, output_dir, "video")
    else:
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ê²½ìš°: ë¹„ë””ì˜¤ íŠ¹ì§• íŒŒì¼ ê²½ë¡œë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
        logger.info("Text-only mode: Using lazy loading for video features...")
        
        # ê¸°ì¡´ ë¹„ë””ì˜¤ íŠ¹ì§• íŒŒì¼ ì°¾ê¸°
        video_feat_file = None
        search_dirs = [
            "/home/work/smoretalk/seo/reranking/new_d/InternVideo/InternVideo2/multi_modality/outputs/feature_result",
            os.path.join(output_dir, "msrvtt_features"),
            "./outputs/msrvtt_features/extract_all_features_20250920_212209/msrvtt_features",
            "./outputs/msrvtt_features"
        ]
        
        for search_dir in search_dirs:
            potential_file = os.path.join(search_dir, "video_features.pkl")
            if os.path.exists(potential_file):
                video_feat_file = potential_file
                break
        
        if not video_feat_file or not os.path.exists(video_feat_file):
            logger.error(f"Video features file not found in any of the search directories")
            logger.error("Search directories:")
            for search_dir in search_dirs:
                logger.error(f"  - {search_dir}")
            logger.error("Please run without text_only mode first to extract video features")
            return
        
        logger.info(f"Video features file found: {video_feat_file}")
        video_metadata = {}  # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ê²½ìš° ë©”íƒ€ë°ì´í„°ëŠ” í•„ìš” ì—†ìŒ
    
    # í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ (ë°°ì¹˜ë³„ ì²˜ë¦¬)
    logger.info("Extracting text features for all videos (batch processing)...")
    # í…ìŠ¤íŠ¸ íŠ¹ì§•ì€ ì¦‰ì‹œ ì €ì¥í•˜ë¯€ë¡œ ë”•ì…”ë„ˆë¦¬ ë¶ˆí•„ìš”
    # text_features = {}
    # text_metadata = {}
    
    # ë¹„ë””ì˜¤ë³„ë¡œ í…ìŠ¤íŠ¸ ê·¸ë£¹í™”
    video_texts = defaultdict(list)
    for text_item in dataset.text_data:
        video_texts[text_item['video_id']].append(text_item)
    
    # ëª¨ë“  ë¹„ë””ì˜¤ IDë“¤ ì²˜ë¦¬ (ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½)
    if text_only_mode:
        # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” ê²½ìš°: ëª¨ë“  ë¹„ë””ì˜¤ IDë¥¼ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¤ê¸°
        all_video_ids = list(set(item['video_id'] for item in dataset.text_data))
    else:
        # ë¹„ë””ì˜¤ íŠ¹ì§•ë„ ì¶”ì¶œí•˜ëŠ” ê²½ìš°: ê¸°ì¡´ ë°©ì‹
        all_video_ids = list(video_features.keys())
    
    batch_size = getattr(config, 'text_batch_size', 3)  # í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë°°ì¹˜ í¬ê¸° (ê·¹ë„ë¡œ ì‘ê²Œ)
    
    for batch_start in tqdm(range(0, len(all_video_ids), batch_size), desc="Processing text batches"):
        batch_end = min(batch_start + batch_size, len(all_video_ids))
        batch_video_ids = all_video_ids[batch_start:batch_end]
        
        logger.info(f"Processing text batch {batch_start//batch_size + 1}: videos {batch_start}-{batch_end-1}")
        
        # ë¹„ë””ì˜¤ íŠ¹ì§•ì€ í•˜ë‚˜ì”© ë¡œë“œí•˜ë¯€ë¡œ ë°°ì¹˜ ë¡œë”© ì œê±°
        
        for video_idx, video_id in enumerate(batch_video_ids):
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ë¯€ë¡œ ë¹„ë””ì˜¤ íŠ¹ì§• ë¶ˆí•„ìš”
                texts = video_texts[video_id]
                # ìº¡ì…˜ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (test/train êµ¬ë¶„)
                captions = []
                for item in texts:
                    caption = item['caption']
                    split = item.get('split', 'unknown')
                    
                    if isinstance(caption, str):
                        # test ë°ì´í„°: ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        if split == 'test':
                            captions.append(caption)
                        else:
                            # train ë°ì´í„°: ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ ì‹œë„
                            try:
                                caption_list = ast.literal_eval(caption)
                                if isinstance(caption_list, list):
                                    captions.extend(caption_list)
                                else:
                                    captions.append(caption)
                            except:
                                # ë³€í™˜ ì‹¤íŒ¨ì‹œ ë‹¨ì¼ ìº¡ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                                captions.append(caption)
                    elif isinstance(caption, list):
                        # train ë°ì´í„°: ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        captions.extend(caption)
                    else:
                        # ê¸°íƒ€ ê²½ìš°: ë¬¸ìì—´ë¡œ ë³€í™˜
                        captions.append(str(caption))
                
                if captions:  # ìº¡ì…˜ì´ ìˆëŠ” ê²½ìš°ë§Œ ì²˜ë¦¬
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    torch.cuda.empty_cache()
                    
                    features = extract_text_features(model, captions, tokenizer, config.device)
                    
                    if features is not None:
                        # ì¦‰ì‹œ ì €ì¥í•˜ê³  ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
                        _save_single_text_feature(video_id, features, texts, output_dir)
                        logger.info(f"Extracted and saved {len(captions)} text features for video {video_id}")
                        
                        # í…ìŠ¤íŠ¸ íŠ¹ì§• ë©”ëª¨ë¦¬ í•´ì œ
                        del features
                    else:
                        logger.warning(f"No valid text features extracted for video {video_id}")
                else:
                    logger.warning(f"No captions found for video {video_id}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache()
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ 10ê°œ ë¹„ë””ì˜¤ë§ˆë‹¤)
            if (video_idx + 1) % 10 == 0:
                torch.cuda.empty_cache()
                logger.info(f"Memory cleanup after processing {video_idx + 1} videos in current batch")
        
        # ë°°ì¹˜ë³„ ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½) - í…ìŠ¤íŠ¸ íŠ¹ì§•ì€ ì´ë¯¸ ê°œë³„ ì €ì¥ë¨
        if batch_start % (batch_size * 2) == 0:  # 2ë°°ì¹˜ë§ˆë‹¤ ë¡œê·¸
            logger.info(f"Completed batch {batch_start//batch_size + 1}")
    
    # ê²°ê³¼ ì €ì¥
    output_dir = os.path.join(config.output_dir, "msrvtt_features")
    os.makedirs(output_dir, exist_ok=True)
    
    # ë¹„ë””ì˜¤ íŠ¹ì§• ì €ì¥ (text_only_modeê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
    if not text_only_mode:
        video_feat_file = os.path.join(output_dir, "video_features.pkl")
        with open(video_feat_file, 'wb') as f:
            pickle.dump(video_features, f)
        
        video_meta_file = os.path.join(output_dir, "video_metadata.pkl")
        with open(video_meta_file, 'wb') as f:
            pickle.dump(video_metadata, f)
    
    # í…ìŠ¤íŠ¸ íŠ¹ì§• ì €ì¥
    text_feat_file = os.path.join(output_dir, "text_features.pkl")
    with open(text_feat_file, 'wb') as f:
        pickle.dump(text_features, f)
    
    text_meta_file = os.path.join(output_dir, "text_metadata.pkl")
    with open(text_meta_file, 'wb') as f:
        pickle.dump(text_metadata, f)
    
    # í†µê³„ ì •ë³´ ì €ì¥
    stats = {
        'total_videos': len(dataset.video_ids),
        'extracted_videos': len(video_features) if not text_only_mode else len(all_video_ids),
        'total_texts': len(dataset.text_data),
        'extracted_texts': sum(len(metadata) for metadata in text_metadata.values()),
        'video_feature_dim': list(video_features.values())[0].shape[1] if video_features and not text_only_mode else 0,
        'text_feature_dim': list(text_features.values())[0].shape[1] if text_features else 0,
        'text_only_mode': text_only_mode
    }
    
    stats_file = os.path.join(output_dir, "extraction_stats.json")
    with open(stats_file, 'w') as f:
        json.dump(stats, f, indent=2)
    
    logger.info(f"âœ… Feature extraction completed!")
    logger.info(f"ğŸ“Š Videos: {stats['extracted_videos']}/{stats['total_videos']}")
    logger.info(f"ğŸ“ Texts: {stats['extracted_texts']}/{stats['total_texts']}")
    logger.info(f"ğŸ’¾ Results saved to {output_dir}")

if __name__ == "__main__":
    # ê¸°ë³¸ config íŒŒì¼ ì„¤ì •
    if len(sys.argv) == 1:
        sys.argv.extend([
            'scripts/evaluation/stage2/zero_shot/6B/config_msrvtt.py',
            'output_dir', './outputs/msrvtt_features/extraction',
            'evaluate', 'True',
            'pretrained_path', '/home/work/smoretalk/seo/reranking/new_d/InternVideo2-Stage2_6B-224p-f4/internvideo2-s2_6b-224p-f4_with_audio_encoder.pt'
        ])
    
    # text_only ì˜µì…˜ í™•ì¸
    text_only = '--text_only' in sys.argv
    if text_only:
        sys.argv.remove('--text_only')
    
    config = setup_main()
    
    # text_only ì˜µì…˜ì„ configì— ì¶”ê°€
    config.text_only = text_only
    
    main(config)
